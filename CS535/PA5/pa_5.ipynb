{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2toXNI25yzV"
      },
      "source": [
        "# CS535/EE514 Machine Learning - Spring 2024 - Assignment 5\n",
        "\n",
        "# Support Vector Machines\n",
        "\n",
        "**Date Assigned**: Saturday, April 06, 2024\n",
        "\n",
        "**Date Due**: Monday, April 22, 2024 (11:55 pm)\n",
        "\n",
        "**Important Notes**\n",
        "\n",
        "1.   The assignment integrates tasks spanning methods as well as principles. Some tasks will involve implementation (in Python) and some may require mathematical analysis.  \n",
        "2.   All cells must be run once before submission and should be displaying the results(graphs/plots etc). Failure to do so will result in deduction of points.\n",
        "3.   While discussions with your peers is strongly encouraged, please keep in mind that the assignment is to be attempted on an individual level. Any plagiarism (from your peers) will be referred to the DC without hestitation.\n",
        "4. For tasks requiring mathematical analysis, students familiar with latex may type their solutions directly in the appropriate cells of this notebook. Alternatively, they may submit a hand-written solution as well.\n",
        "5. Use procedural programming style and comment your code properly.\n",
        "5. Upload your solutions as a zip folder with name `RollNumber_A5` on the Assignment tab and submit your hand-written solutions in the drop-box next to the instructor's office.\n",
        "5. **Policy on Usage of Generative AI Tools**. Students are most welcome to use generative AI tools as partners in their learning journey. However, it should be kept in mind that these tools cannot be blindly trusted for the tasks in this assignment (hopefully) and therefore it is important for students to rely on their own real intelligence (pun intended) before finalizing their solution/code. It is also mandatory for students to write a statement on how exactly have they used any AI tool in completing this assignment.\n",
        "5. **Vivas** The teaching staff reserves the right to conduct a viva for any student.   \n",
        "5. **Policy on Late Submission**. Late solutions will be accepted with a 15% penalty per day till Wednesday, April 24, 2024 (11:55 pm) . No submissions will be accepted after that.      \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMR9esF1ZmX0"
      },
      "source": [
        "## Task 1. Support Vector Machines Implementation\n",
        "\n",
        "All the necessary imports have been given below. If you are running the assignment locally, you may comment out the pip command below to install the QP solver for SVMs. If you need to import anything else, feel free to reach out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "wsjDVfBoZmX6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: cvxopt in c:\\users\\haroo\\anaconda3\\lib\\site-packages (1.3.2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import matplotlib as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "!pip install cvxopt\n",
        "import cvxopt # The optimization package for Quadratic Programming\n",
        "import cvxopt.solvers\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-5odcOjZmX-"
      },
      "source": [
        "We will be using the make moon dataset for this assignment. We will be first implementing it with the help of library functions and then through our own implementation and compare the results. The following code snippet has been provided for you which divides the dataset into train and test. Please do not modify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "qo3Y-hiFZmX_"
      },
      "outputs": [],
      "source": [
        "X, y = make_moons (n_samples = 500, noise = 0.15, random_state = 42)\n",
        "y = y*2-1.0\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHZMM58nZmYA"
      },
      "source": [
        "## Visualization function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "CW6gPp6AZmYB"
      },
      "outputs": [],
      "source": [
        "def visualization_function (classifier, feature_matrix, label_vector, axes=[-2, 3, -2, 2]):\n",
        "    \"\"\"\n",
        "    Generate a simple plot of SVM including the decision boundary, margin, and its training data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    classifier: your classifier handle\n",
        "    feature_matrix: feature matrix shape(m_samples, n_features)\n",
        "    label_vector: label vector shape(m_samples, )\n",
        "    axes: (optional) the axes of the plot in format [xmin, xmax, ymin, ymax]\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a mesh grid based on the provided axes (100 x 100 resolution)\n",
        "    axes_x = np.linspace(axes[0], axes[1], 100)\n",
        "    axes_y = np.linspace(axes[2], axes[3], 100)\n",
        "    x, y = np.meshgrid(axes_x, axes_y)\n",
        "\n",
        "    # predicting decision boundary\n",
        "    x_new = np.c_(x.ravel(), y.ravel())\n",
        "    y_pred = classifier.predict(x_new).reshape(x.shape)\n",
        "\n",
        "    # Plot decision boundary and margins\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.contour(x, y, y_pred, alpha=0.2, cmap=plt.cm.brg)\n",
        "\n",
        "    # Plot out the support vectors\n",
        "    support_vectors = classifier.support_vectors_\n",
        "    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], linewidth=1, facecolors='none', edgecolors='k')\n",
        "\n",
        "    # Plot out the training data\n",
        "    plt.scatter(feature_matrix[:, 0], feature_matrix[:, 1], c=label_vector, cmap=plt.cm.brg, edgecolors='k')\n",
        "\n",
        "    plt.title(\"SVM plot with decision boundary, margin, and its training data\")\n",
        "    plt.xlable('Feature 1')\n",
        "    plt.ylable('Feature 2')\n",
        "    plt.axis(axes)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6k05yW8ZmYC"
      },
      "source": [
        "## SVM Classifier Implementation and Evaluation using Built in Library SVC\n",
        "\n",
        "### Kernels to Implement\n",
        "- **Linear Kernel**\n",
        "- **Polynomial Kernel**\n",
        "- **Gaussian Radial Basis Function (RBF) Kernel**\n",
        "\n",
        "### Procedure\n",
        "\n",
        "#### 1. Hyperparameter Optimization\n",
        "- Utilize grid search to fine-tune the model parameters.\n",
        "- Focus on the regularization parameter `C` and the kernel-specific parameter `gamma` (where applicable).\n",
        "\n",
        "#### 2. Model Training and Evaluation\n",
        "- Train the SVM classifier using the hyperparameters obtained from the optimization step.\n",
        "- Evaluate the trained model on a separate test dataset to determine its performance.\n",
        "\n",
        "#### 3. Metrics Reporting\n",
        "- Compile and report the following metrics to assess the classifier's performance:\n",
        "  - Confusion matrix\n",
        "  - Recall\n",
        "  - Precision\n",
        "\n",
        "#### 4. Visualization\n",
        "- Use the predefined `visualization_function()` function to visualize the SVMâ€™s decision boundaries and support vectors in 2D.\n",
        "  This will provide insights into how the SVM constructs the decision plane and identifies support vectors across different kernels.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "2ynp02R1ZmYD"
      },
      "outputs": [],
      "source": [
        "def svm_evaluation_library_functions(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Tune and evaluate SVM with different kernels.\n",
        "    1. Define the parameter grid for each kernel type.\n",
        "    2. Perform grid search to find the best hyperparameters.\n",
        "    3. Train the model using the best hyperparameters and evaluate it.\n",
        "    4. Display the evaluation metrics and visualize the SVM.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1.\n",
        "    parameter_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf', 'poly']\n",
        "    }\n",
        "\n",
        "    # 2.\n",
        "    grid_search = GridSearchCV(SVC(), param_grid=parameter_grid, scoring='accuracy', cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # 3.\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # 4.\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "    if X_train.shape[1] == 2:\n",
        "        pass\n",
        "        # visualization_function(best_model, X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGMgGQ2DZmYF"
      },
      "source": [
        "## Custom non Linear SVM implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "NmdF56XdZmYG"
      },
      "outputs": [],
      "source": [
        "def linear_kernel(vector_a, vector_b):\n",
        "    \"\"\"\n",
        "    Computes the linear kernel between two vectors.\n",
        "    1. The function should take two vectors, vector_a and vector_b, as input.\n",
        "    2. It calculates the dot product of these vectors.\n",
        "    3. The result is returned as the linear kernel value.\n",
        "    \"\"\"\n",
        "    # making sure they are numpy arrays\n",
        "    vector_a = np.array(vector_a)\n",
        "    vector_b = np.array(vector_b)\n",
        "\n",
        "    # just the dot product between the input vectors\n",
        "    linear_kernel_value = np.dot(vector_a, vector_b)\n",
        "\n",
        "    return linear_kernel_value\n",
        "    \n",
        "def polynomial_kernel(vector_a, vector_b, degree=None):\n",
        "    \"\"\"\n",
        "    Computes the polynomial kernel between two vectors.\n",
        "    1. The function takes two vectors, vector_a and vector_b, and the polynomial degree as input.\n",
        "    2. It calculates the polynomial kernel by raising the dot product of vector_a and vector_b to the specified power of degree.\n",
        "    3. The result is returned as the polynomial kernel value, indicating the similarity in a polynomial feature space.\n",
        "    \"\"\"\n",
        "    # making sure they are numpy arrays\n",
        "    vector_a = np.array(vector_a)\n",
        "    vector_b = np.array(vector_b)\n",
        "\n",
        "    # calculating polynomial kernel\n",
        "    polynomial_kernel_value = 1 + np.dot(vector_a, vector_b)\n",
        "    if degree:\n",
        "        polynomial_kernel_value = polynomial_kernel_value ** degree\n",
        "    \n",
        "    return polynomial_kernel_value\n",
        "\n",
        "def rbf_kernel(vector_a, vector_b, gamma=None):\n",
        "    \"\"\"\n",
        "    Computes the Gaussian RBF kernel between two vectors.\n",
        "    1. The function takes two vectors, vector_a and vector_b, and the parameter gamma as input.\n",
        "       Gamma is a key parameter in the RBF kernel which determines the influence of individual training samples\n",
        "       on the decision boundary. Specifically, it controls the width of the Gaussian function:\n",
        "       - A low gamma value makes the decision boundary smoother, with a wider spread of influence for each support vector.\n",
        "       - A high gamma value creates a more complex model, where influence is limited to vectors close to the support vector,\n",
        "         leading to a decision boundary that closely follows the training points.\n",
        "    2. It calculates the Gaussian RBF kernel using the squared Euclidean distance between vector_a and vector_b, scaled by gamma.\n",
        "    3. The result, an exponential function of the negative scaled squared distance, is returned as the RBF kernel value,\n",
        "       depicting the similarity in the RBF feature space.\n",
        "    \"\"\"\n",
        "    # making sure they are numpy arrays\n",
        "    vector_a = np.array(vector_a)\n",
        "    vector_b = np.array(vector_b)\n",
        "\n",
        "    # calculating kernel\n",
        "    gaussian_rbf_kernel = (np.linalg.norm(vector_a - vector_b)**2)\n",
        "    if gamma:\n",
        "        gaussian_rbf_kernel *= (-gamma)\n",
        "    rbf_kernel_value = np.exp(gaussian_rbf_kernel)\n",
        "\n",
        "    return rbf_kernel_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYt9WXk3ZmYH"
      },
      "source": [
        "## Implementing the SVM Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "gmnHKMRkZmYH"
      },
      "outputs": [],
      "source": [
        "class NonLinearSVM (BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Custom implementation of the Support Vector Machine (SVM) algorithm\n",
        "    The class utilizes the quadratic programming solver from cvxopt to optimize the SVM formulation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel=None, regularization_parameter=None):\n",
        "        \"\"\"\n",
        "        Initializes the NonLinearSVM instance with the specified kernel function and regularization parameter.\n",
        "\n",
        "        Parameters:\n",
        "        kernel: A function that computes the kernel between two data points. The kernel function defines the feature\n",
        "            space in which the dataset is transformed, allowing SVM to perform classification in this space.\n",
        "        regularization_parameter: The regularization parameter C controls the trade-off between achieving a low error on the\n",
        "            training data and minimizing the model complexity for better generalization. If C is set to None,\n",
        "            it implies an infinite margin (hard margin SVM). A larger C tries to classify all training examples\n",
        "            correctly (hard margin), while a smaller C allows more misclassifications but might lead to a better generalization (soft margin).\n",
        "        \"\"\"\n",
        "        if kernel == \"rbf\":\n",
        "            self.kernel = rbf_kernel\n",
        "        elif kernel == \"poly\":\n",
        "            self.kernel = polynomial_kernel\n",
        "        elif kernel == \"linear\":\n",
        "            self.kernel = linear_kernel\n",
        "\n",
        "        self.C = regularization_parameter\n",
        "\n",
        "    def fit(self, feature_matrix, target_vector):\n",
        "        \"\"\"\n",
        "        Trains the SVM model using the provided training data.\n",
        "        Parameters:\n",
        "        feature_matrix : array-like, shape (total_num_samples, total_num_features)\n",
        "            The training input samples.\n",
        "        target_vector : array-like, shape (total_num_samples,)\n",
        "            The target values (class labels) relative to the input samples.\n",
        "\n",
        "        The training process involves several steps:\n",
        "        a. It computes the kernel matrix, where each element is the result of applying the kernel function to a pair of samples.\n",
        "        b. It sets up and solves a quadratic programming problem to find the Lagrange multipliers.\n",
        "        c. It identifies the support vectors as those samples corresponding to non-zero Lagrange multipliers.\n",
        "        d. In the case of a linear kernel, it computes the weight vector w. For non-linear kernels, w is not used.\n",
        "        e. It computes the intercept term b using the support vectors and their properties.\n",
        "\n",
        "        Starter code has been provided for this. Fill in the dotted lines and code below each comment provided.\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute the number of samples and features\n",
        "        total_num_samples, total_num_features = feature_matrix.shape\n",
        "\n",
        "        # a. Compute the kernel matrix for all pairs of training examples\n",
        "        # Kernel function is applied to each pair of samples in the training set\n",
        "        K = np.zeros((total_num_samples, total_num_features))\n",
        "        for i in range(total_num_samples):\n",
        "            for j in range(total_num_features):\n",
        "                K[i, j]  =  self.kernel(feature_matrix[i], feature_matrix[j])  \n",
        "\n",
        "        # b. Setup the Quadratic Programming (QP) problem for SVM\n",
        "        \n",
        "        # quadratic_term: Quadratic term in the objective, derived from the outer product of the target vector and kernel matrix\n",
        "        # quadratic_term = cvxopt.matrix(np.outer(target_vector, target_vector) * K)  # P\n",
        "        quadratic_term = cvxopt.matrix(np.outer(target_vector, target_vector))  # P\n",
        "        P = quadratic_term\n",
        "\n",
        "        # linear_term: Linear term in the objective, a vector of -1's\n",
        "        linear_term = cvxopt.matrix(-np.ones((total_num_samples, 1)))   # q\n",
        "        q = linear_term\n",
        "\n",
        "        # equality_constraint_coeff: Equality constraint coefficient for the problem, converting target vector to meet cvxopt requirements\n",
        "        equality_constraint_coeff = cvxopt.matrix(target_vector.reshape(1, -1)) # A\n",
        "        A = equality_constraint_coeff\n",
        "\n",
        "        # equality_constraint_threshold: Equality constraint threshold, set to 0 to enforce the constraint that sum(alpha_i * y_i) = 0\n",
        "        equality_constraint_threshold = cvxopt.matrix(np.zeros(1))  # b\n",
        "        b = equality_constraint_threshold\n",
        "\n",
        "        # For the SVM problem, constraints are defined for Lagrange multipliers (alpha)\n",
        "        # For hard margin SVM, constraints ensure alpha >= 0\n",
        "        # For soft margin SVM (when C is defined), constraints allow 0 <= alpha <= C\n",
        "        # You will define two variables for this: an inequality constraint matrix and an inequality constraint vector.\n",
        "        # You will check whether the regularization parameter has been defined to see if its hard or soft margin.\n",
        "\n",
        "        # G & h\n",
        "        if self.C:\n",
        "            inequality_constraint_matrix = cvxopt.matrix(np.vstack((-np.eye(total_num_samples), np.eye(total_num_samples))))\n",
        "            inequality_constraint_vector = cvxopt.matrix(np.hstack((np.zeros(total_num_samples), np.ones(total_num_samples) * self.C)))\n",
        "        else:\n",
        "            inequality_constraint_matrix = cvxopt.matrix(-np.eye(total_num_samples))\n",
        "            inequality_constraint_vector = cvxopt.matrix(np.zeros(total_num_samples))\n",
        "\n",
        "        G = inequality_constraint_matrix\n",
        "        h = inequality_constraint_vector\n",
        "\n",
        "        # Solve the quadratic programming problem using cvxopt\n",
        "        cvxopt.solvers.options['show_progress'] = False\n",
        "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
        "\n",
        "        # Extract the Lagrange multipliers from the solution\n",
        "        alphas = np.ravel(solution['x'])\n",
        "\n",
        "        # Support vectors have non-zero Lagrange multipliers\n",
        "        support_vectors = (alphas > 0).flatten()\n",
        "\n",
        "        # Identify these support vectors for later use in the model\n",
        "        indices = np.arange(len(alphas))[support_vectors]\n",
        "        self.alphas = alphas[support_vectors]\n",
        "        self.sv = feature_matrix[support_vectors]\n",
        "        self.sv_y = target_vector[support_vectors]\n",
        "\n",
        "        # Code part d and e here\n",
        "\n",
        "        # d & E \n",
        "        if self.kernel == 'linear':\n",
        "            self.w = np.dot((target_vector * alphas).T @ feature_matrix).reshape(-1, 1)\n",
        "            self.b = np.mean(self.sv_y - np.dot(self.sv, self.w))\n",
        "        else:\n",
        "            self.w = None\n",
        "            self.b = np.mean(self.sv_y - np.sum(self.sv * self.alphas * K[indices][:, self.sv], axis=1))\n",
        "\n",
        "def decision_function(self, feature_matrix):\n",
        "    \"\"\"\n",
        "    Calculates the signed distance of samples in feature_matrix from the decision boundary defined by the SVM model.\n",
        "\n",
        "    Parameters:\n",
        "    feature_matrix : array-like, shape (total_num_samples, total_num_features)\n",
        "        Feature matrix for which to predict the distances from the decision boundary.\n",
        "\n",
        "    Returns:\n",
        "    distances : array, shape (total_num_samples,)\n",
        "        The signed distances of each sample in feature_matrix from the decision boundary.\n",
        "\n",
        "    The decision function is defined differently based on the kernel used:\n",
        "    - For a linear kernel, it's the dot product of the input features and the weight vector plus the intercept.\n",
        "    - For non-linear kernels, the decision function involves applying the kernel function between the input features\n",
        "      and the support vectors, weighted by the learned Lagrange multipliers and the labels of the support vectors.\n",
        "\n",
        "    Specifically:\n",
        "    - If the linear kernel is used (self.w is not None), the decision function calculates as w^T * feature_matrix + b.\n",
        "    - For non-linear kernels, the function sums the product of the Lagrange multipliers (self.a),\n",
        "      the labels of the support vectors (self.y), and the kernel function applied between each support vector\n",
        "      and the input features. The intercept (self.b) is added to this sum to obtain the final decision function value.\n",
        "    \"\"\"\n",
        "    if self.kernel == linear_kernel:\n",
        "        return np.dot(self.w.T, feature_matrix) + self.b\n",
        "    else:\n",
        "        sum_ = np.sum(self.a * self.y * self.kernel)\n",
        "        return  sum_ + self.b\n",
        "\n",
        "def predict(self, X):\n",
        "    \"\"\"\n",
        "    The predict method classifies samples in X based on the sign of the decision function, outputting -1 or +1.\n",
        "    \"\"\"\n",
        "    decision = self.decision_function(X)\n",
        "\n",
        "    class_ = np.sign(decision).astype(int)\n",
        "    return class_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZYycP5xZmYI"
      },
      "source": [
        "## Using the nonLinearSVM class to train the models and generating a classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "V_slKFKvZmYI"
      },
      "outputs": [],
      "source": [
        "def train_evaluate_plot_svm(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Trains and evaluates the custom SVM with different kernels.\n",
        "    1. Create MySVM instances with linear, polynomial, and RBF kernels.\n",
        "    2. Use the best hyperparameters found through the library implementation.\n",
        "    3. Train the SVM on the training data using these hyperparameters.\n",
        "    4. Evaluate the trained SVM on the test data and print the precision, accuracy, and recall.\n",
        "    5. Use the visualization function to plot the decision boundaries.\n",
        "    \"\"\"\n",
        "    kernels = ['linear', 'polynomial', 'rbf']\n",
        "    params = {\n",
        "        'C': [1, 10, 100],\n",
        "        'kernel': kernels,\n",
        "        'degree': [2, 3],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(SVC(), params, cv=3, scoring='accuracyr')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "    if X_train.shape[1] == 2:\n",
        "        visualization_function(best_model, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy on test data: 0.98\n"
          ]
        }
      ],
      "source": [
        "# initializing the kernel\n",
        "model_ = SVC(kernel='rbf', C=1.0, gamma='scale') \n",
        "\n",
        "# training the model\n",
        "model_.fit(X_train, y_train)\n",
        "y_pred = model_.predict(X_test)\n",
        "\n",
        "# calculating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Model accuracy on test data: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall: 1.00\n",
            "Precision: 1.00\n",
            "Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            " [[43  0]\n",
            " [ 0 57]]\n"
          ]
        }
      ],
      "source": [
        "# initializing the kernel\n",
        "model__ = NonLinearSVM(kernel='rbf', regularization_parameter=1.0) \n",
        "\n",
        "# training the model\n",
        "model_.fit(X_train, y_train)\n",
        "y_pred = model_.predict(X_test)\n",
        "\n",
        "# calculating accuracy\n",
        "svm_evaluation_library_functions(X_train, y_train, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFX7qJkpZmYJ"
      },
      "source": [
        "## Comparison Questions\n",
        "\n",
        "1. How does the accuracy of your custom SVM implementation compare with scikit-learn's SVC?\n",
        "    \n",
        "    Comparing the accuracy of a custom SVM implementation with scikit-learn's SVC typically reveals that scikit-learn's implementation achieves higher accuracy in most cases. This difference in accuracy can arise due to various factors such as optimization techniques, handling of numerical stability, and parameter tuning.\n",
        "2. How sensitive is each model to hyperparameter changes, particularly C and gamma? Perform a detailed analysis by varying these parameters and observing the impact on performance metrics like accuracy, precision, and recall. \n",
        "\n",
        "Both the custom SVM implementation and scikit-learn's SVC are sensitive to hyperparameter changes, particularly the regularization parameter C and the kernel parameter gamma. Varying these parameters can have a significant impact on model performance metrics such as accuracy, precision, and recall. Typically, increasing C n both implementations leads to a tighter decision boundary and potentially overfitting, while increasing gamma an make the decision boundary more complex and prone to overfitting in the case of kernel SVMs.\n",
        "\n",
        "3. Investigate how the selection of kernel parameters (like degree in the polynomial kernel) affects the decision boundary in both the custom and scikit-learn SVMs. Can you visualize the differences in decision boundaries for various parameter settings?\n",
        "\n",
        "The selection of kernel parameters, such as the degree in the polynomial kernel, significantly affects the decision boundary in both custom and scikit-learn SVMs. Higher degrees in polynomial kernels can result in more complex decision boundaries, which may lead to overfitting if not properly tuned. Visualizing the decision boundaries for various parameter settings can provide insights into how the models generalize and separate classes in the feature space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW-uu4BSh5ct"
      },
      "source": [
        "## Task 2. Multi-Class SVM\n",
        "\n",
        "In order to extend the binary SVM classifier discussed in class to $C$ classes, consider the following one-versus rest proposal.\n",
        "\n",
        "Train a binary SVM (possibly non-linear) classifier by first treating class-$0$  points on one side and the remaining on the other. Let the hypothesis function be denoted by $h_0(.)$. Thus, for a test point $\\vec{x}_t$, the function $h_0(\\vec{x}_t)$ returns a 1 if it thinks the point is class-0 and returns a 0 otherwise.\n",
        "\n",
        "Repeat the procedure for every class to ultimately obtain $C$ hypothesis functions $h_0(.), h_1(.), \\ldots, h_{C-1}(.)$.\n",
        "\n",
        "Armed with these hypothesis functions, Khurram claims that the multi-class SVM formulation is complete: To determine whether a test point belongs to any class-$i$ or not, he claims he will simply evaluate $\\vec{x}_t$, he claims he will evaluate $h_i(\\vec{x}_t)$ and if it returns 1, we know the point is class-$i$ - Mission accomplished.\n",
        "\n",
        "1. Explain why Khurram's classifier will not always work.\n",
        "2. Devise a solution to overcome the problem you identify with Khurram's method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image.png](task2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Nziw7Xyq5K"
      },
      "source": [
        "## Task 3. Kernelized Perceptron\n",
        "\n",
        "Recall that we could induce non-linear boundaries with SVM by projecting the features $\\vec{x}$ to the vector $\\phi(\\vec{x})$ that resides in a higher dimensional space. To reduce the computational complexity associated with computing inner products in the high dimensional space, we resort to using kernel functions $k(\\vec{x}_i, \\vec{x}_j) = \\phi(\\vec{x}_i)^T \\phi(\\vec{x}_j)$.\n",
        "\n",
        "The same idea could be used to induce a non-linear classifer with Perceptron as well: Map all $\\vec{x}_i$ to the vector $\\phi(\\vec{x}_i)$ and then determine a linear decision boundary in the higher dimensions - the linear boundary in the higher dimensions would induce a non-linear one in the original feature space. To reduce the computational complexity, we would however like to resort to using kernel functions for Perceptron as well. This task will have you derive a kernelized version of the Perceptron algorithm.\n",
        "\n",
        "1. With the initialization $\\vec{w} = 0$, show that at any iteration, $\\vec{w} = \\sum_{i=j}^N \\alpha_j \\phi(\\vec{x}_j)$.\n",
        "2. Show that as a consequence of point 1 above, we have\n",
        "\n",
        "$$\\vec{w}^T \\vec{x}_i = \\sum_{j=1}^N \\alpha_j k(\\vec{x}_j, \\vec{x}_i) $$\n",
        "\n",
        "Thus, without even knowing the transformation, the inner product can be computed if we just keep track of and update the weights $\\alpha_1, \\ldots, \\alpha_N$.\n",
        "\n",
        "3. Given the information above, write a Pseudo-code (similar to the one in class notes) for a kernelized version of Perceptron.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image.png](task3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BtRaZ9SK0IB"
      },
      "source": [
        "## Task 4.\n",
        "\n",
        "Consider the following hard-SVM problem formulation. A SVM classifier aiming to maximize the margin is defined by the optimization problem:\n",
        "\n",
        "$$\n",
        "\\min_{\\vec{w}, b} \\|\\vec{w}\\|^2 \\quad\n",
        "$$\n",
        "\n",
        "subject to the constraints:\n",
        "\n",
        "$$\n",
        "y_i(\\vec{w}^T \\vec{x}_i + b) \\geq 1, \\quad \\forall \\quad i=1,\\ldots,N \\quad\n",
        "$$\n",
        "\n",
        "\n",
        "Suppose that the constraints are now modified to\n",
        "\n",
        "$$\n",
        "y_i(\\vec{w}^T \\vec{x}_i + b) \\geq c, \\quad \\forall \\quad i=1,\\ldots,N \\quad\n",
        "$$\n",
        "\n",
        "for any positive constant $c$. Show that the maximum margin hyperplane obtained with this modification will exactly be the same as the one obtained without the modification. In other words, the optimization result is independent of $c$.\n",
        "\n",
        "*Hint*. Refer to the steps followed in class in deriving the optimization problem above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](task4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNBR0oG5Te2L"
      },
      "source": [
        "## Task 5.\n",
        "\n",
        "Consider the following soft-SVM problem formulation. The SVM classifier is defined by the optimization problem:\n",
        "\n",
        "$$\n",
        "\\min_{\\vec{w}, b} \\|\\vec{w}\\|^2 + C \\sum_{i=1}^N \\zeta_i\n",
        "$$\n",
        "\n",
        "subject to the constraints:\n",
        "\n",
        "$$\n",
        "y_i(\\vec{w}^T \\vec{x}_i + b) \\geq 1 - \\zeta_i, \\quad \\forall \\quad i=1,\\ldots,N \\quad\n",
        "$$\n",
        "\n",
        "$$ \\zeta_i \\geq 0 \\quad \\forall \\quad i=1,\\ldots,N \\quad$$\n",
        "\n",
        "Where $C \\geq 0$ is a hyperparameter that controls the tradeoff between the penalty and the margin term. Why does $C$ have to be non-negative? Answer this by stating how the optimization problem will behave when $C < 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image Alt Text](task5.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
